import pandas as pd
import numpy as np
import xgboost as xgb
import shap
import matplotlib.pyplot as plt
import os
import joblib
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from skopt import BayesSearchCV
from skopt.space import Real, Integer
from scipy.stats import linregress
from xgboost import XGBRegressor

# Load datasets
train_df = pd.read_excel("C:/....../....../...../......")
test_df = pd.read_excel("C:/....../....../...../......")

# Splitting data into features and target
X_train = train_df.drop(columns='Y')  # Drop the output column to get the features
y_train = train_df['Y']  # The output column is 'Y'
X_test = test_df.drop(columns='Y')
y_test = test_df['Y']

# Define a function to compute performance metrics
def compute_metrics(y_true, y_pred):
    return {
        'R2': r2_score(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'MAE': mean_absolute_error(y_true, y_pred),
        'MAPE': np.mean(np.abs((y_true - y_pred) / y_true)) * 100,
        'MBE': np.mean(y_pred - y_true),
    }

# Define initial hyperparameters
initial_params = {
    'n_estimators': 10,
    'max_depth': 3,
    'learning_rate': 0.1,
    'colsample_bytree': 1.0,
    'subsample': 1.0,
}

# Define XGBoost model with initial hyperparameters
initial_xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, **initial_params)

# Train the initial model
initial_xgb_model.fit(X_train, y_train)

# Predictions and metrics for initial model on training data
initial_predictions_train = initial_xgb_model.predict(X_train)
initial_train_metrics = compute_metrics(y_train, initial_predictions_train)

# Predictions and metrics for initial model on test data
initial_predictions_test = initial_xgb_model.predict(X_test)
initial_test_metrics = compute_metrics(y_test, initial_predictions_test)

# Bayesian optimization setup
search_spaces = {
    'n_estimators': Integer(10, 600),
    'max_depth': Integer(3, 5),
    'learning_rate': Real(0.01, 0.3),
    'colsample_bytree': Real(0.4, 1.0),
    'subsample': Real(0.4, 1.0)
}

opt = BayesSearchCV(initial_xgb_model, search_spaces, n_iter=100, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)
opt.fit(X_train, y_train)

# Best model after optimization
best_model = opt.best_estimator_

# Predictions and metrics for tuned model
predictions_train = best_model.predict(X_train)
predictions_test = best_model.predict(X_test)
train_metrics = compute_metrics(y_train, predictions_train)
test_metrics = compute_metrics(y_test, predictions_test)

# Visualize one of the trees from the best-tuned XGBoost model
plt.figure(figsize=(30, 30))
xgb.plot_tree(best_model, num_trees=0, ax=plt.gca())
plt.savefig('C:/users/asus1/desktop/xgb_best_model_tree.png', dpi=600)
plt.close()

# Create a DataFrame for initial and tuned hyperparameters
    hyperparameters_df = pd.DataFrame({
        'Parameter': list(initial_params.keys()),
        'Initial Value': list(initial_params.values()),
        'Tuned Value': [best_model.get_params()[param] for param in list(initial_params.keys())],
    })
    hyperparameters_df.to_excel(writer, sheet_name='Hyperparameters') 

# SHAP values for feature importance
explainer = shap.Explainer(best_model)
shap_values = explainer(X_train)

# Summary Plot - for each feature
plt.figure(figsize=(25, 25))
shap.summary_plot(shap_values, X_train, feature_names=X_train.columns)

# Bar Plot - showing the average impact of each feature
# Convert column names to a list
feature_names_list = X_train.columns.to_list()
plt.figure(figsize=(25, 25))
shap.summary_plot(shap_values, X_train, plot_type="bar")
plt.close()

# List of input combinations for evaluation
input_combinations = [
    ['X1'],
    ['X1', 'X2'],
    ['X1', 'X2', 'X3'],
    ['X1', 'X2', 'X3', 'X4'],
    ['X1', 'X2', 'X3', 'X4', 'X5'],
    # Add more combinations as needed
]

# Dictionary to store evaluation results for each input combination
directory = "C:/....../....../...../....."
evaluation_results = {}

# Evaluate performance for each input combination
for input_features in input_combinations:
# Subset features
    X_train_subset = X_train[input_features]
    X_test_subset = X_test[input_features]
    
# Train the model on the subset of features
    best_model.fit(X_train_subset, y_train)
    
# Predictions on training and testing data
    predictions_train_subset = best_model.predict(X_train_subset)
    predictions_test_subset = best_model.predict(X_test_subset)
    
# Compute metrics for the subset
    metrics_train_subset = compute_metrics(y_train, predictions_train_subset)
    metrics_test_subset = compute_metrics(y_test, predictions_test_subset)
    
# Compute additional metrics for the subset
    additional_metrics_train_subset = compute_additional_metrics(y_train, predictions_train_subset)
    additional_metrics_test_subset = compute_additional_metrics(y_test, predictions_test_subset)

# Store results
    evaluation_results[str(input_features)] = {
        'Metrics Train': metrics_train_subset,
        'Metrics Test': metrics_test_subset,
        'Additional Metrics Train': additional_metrics_train_subset,
        'Additional Metrics Test': additional_metrics_test_subset
    }
       
# Example prediction using the best XGBoost model with specific inputs
example_input = np.array([[0.5,0.5,2.5,0.5,0.5]])
example_prediction = best_model.predict(example_input)

# Save the best XGBoost model as XGB_GUI.joblib
joblib.dump(best_model, 'C:/....../....../...../......joblib')
